{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hrachkhachatryan/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_data_cleaned_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8371056"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hrachkhachatryan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/hrachkhachatryan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for i, row in df.iterrows():\n",
    "    blob = TextBlob(str(row.message_text))\n",
    "    sentiment = blob.sentiment\n",
    "    df.set_value(i,'polarity', float(sentiment.polarity))\n",
    "    df.set_value(i,'subjectivity', float(sentiment.subjectivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10463820"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 322759\n",
      "neg: 47394\n",
      "neu: 676229\n"
     ]
    }
   ],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "neu = 0\n",
    "for i, row in df.iterrows():\n",
    "    if row.polarity >= 0.2:\n",
    "        pos = pos +1\n",
    "    elif row.polarity <= -0.2:\n",
    "        neg = neg + 1\n",
    "    else:\n",
    "        neu = neu + 1\n",
    "        \n",
    "print('pos:', pos)\n",
    "print('neg:', neg)\n",
    "print('neu:', neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('pos_neg_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    df.set_value(index, 'timestamp', pd.Timestamp(row['timestamp'], unit='s').date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('pos_neg_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_topic_groups = df.groupby([df.topic_id, df.timestamp]).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.pos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_neg_groups = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for titleID, timestamp in date_topic_groups.keys():\n",
    "    pol = 0.\n",
    "    j = len(date_topic_groups[(titleID, timestamp)])\n",
    "    for item in date_topic_groups[(titleID, timestamp)]:\n",
    "        pol += df.polarity[item]\n",
    "    pos_neg_groups.update({(titleID, timestamp):{'posts':date_topic_groups[(titleID, timestamp)], 'pol': pol/j}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(pos_neg_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_topics = df.groupby(df.topic_id).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_topic = {}\n",
    "for tit in tqdm(grouped_topics.keys()):\n",
    "    sentence = (df.message_text[grouped_topics[tit]].values)\n",
    "    dict_topic.update({tit: ''.join(str(v) for v in sentence) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_message = []\n",
    "for i in dict_topic.keys():\n",
    "    list_message.append(dict_topic[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')  #match any word characters until it reaches a non-word character, like a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = list_message[0].lower()\n",
    "tokens = tokenizer.tokenize(raw)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove stop words from tokens\n",
    "stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "# print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_words = ['will','s' , 'nbsp', 't', 'com', 'http', 'amp', '1xnk8bc', 'href', 'oto', 'www' ]\n",
    "stopped_tokens = [i for i in stopped_tokens if not i in remove_words]\n",
    "# print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# Create lemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "remove_words = ['will','s' ,'bitcoin', 'just', 'get', 'use', 'now', 'solidx' 'people','bitfinex', 'think', 'maybe', 'imageshack', 'pt', 'em', 'img', 'nbsp', 't', 'com', 'http', 'amp', '1xnk8bc', 'href', 'oto', 'www', 'isn', 'etc', 'etf', 'tr', 'td', 'img', 'ath', 'xt', 'xp', 'php', 'img', 'gt', 'pboc', 'th', 'mtgox', 'cny', 'huobi',\n",
    " 'm', 'import', 'st', 'lt', 'zhou' 'ok', 'color', 'can', \"adam\", \"bitcoin\", 'import', 'http', 'li', 'b', 'style' , 'font', 're', 'le', 'gif','span','hr', 'd' , 'jpg', 'png',  'am5om', 'fud', 'mt', 'th' 'hfebupaeo', 'ftdata', 'zbb', 'imgur', 'bite', 'uztgwi', 'podomatic']\n",
    "\n",
    "map_words = {\n",
    "    'btc': 'bitcoin',\n",
    "    'bcc': 'bitcoin',\n",
    "    'gbtc': 'bitcoin',\n",
    "    'bitcoinca': 'bitcoin',\n",
    "    'better': 'good',\n",
    "    'increase': 'rise',\n",
    "    'miner': 'mine',\n",
    "    'winner': 'win'\n",
    "}\n",
    "\n",
    "bow_dict = {}\n",
    "for m in dict_topic.keys():\n",
    "    raw = dict_topic[m].lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    \n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    stopped_tokens = [i for i in stopped_tokens if not i in remove_words]\n",
    "\n",
    "    #   lemmatize tokents:  \n",
    "    lemmatized_tokens = [wnl.lemmatize(i, 'a') for i in stopped_tokens]\n",
    "    lemmatized_tokens = [wnl.lemmatize(i, 'n') for i in lemmatized_tokens]\n",
    "    lemmatized_tokens = [wnl.lemmatize(i, 'v') for i in lemmatized_tokens]\n",
    "    lemmatized_tokens = [wnl.lemmatize(i, 'r') for i in lemmatized_tokens]\n",
    "    lemmatized_tokens = [wnl.lemmatize(i, 's') for i in lemmatized_tokens]\n",
    "    \n",
    "    #     stemmed_tokens = [p_stemmer.stem(i) for i in lemmatized_tokens]\n",
    "    \n",
    "    #   remove words:\n",
    "    stemmed_tokens = [i for i in lemmatized_tokens if not i in remove_words]\n",
    "    stemmed_tokens = [i for i in stemmed_tokens if not i in en_stop]\n",
    "    d = pd.DataFrame({'z': stemmed_tokens})\n",
    "    stemmed_tokens = d.replace(map_words)['z'].tolist()\n",
    "    \n",
    "    #   remove numeric values:\n",
    "    alpha_only = [''.join(filter(str.isalpha, i))  for i in stemmed_tokens if len(''.join(filter(str.isalpha, i))) >1]\n",
    "    bow_dict.update({m:alpha_only})\n",
    "    texts.append(alpha_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import datetime\n",
    "\n",
    "a = datetime.datetime.now()\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep bag of words in order to get docements for each of the topic\n",
    "for i in bow_dict.keys():\n",
    "    bow = dictionary.doc2bow(bow_dict[i])\n",
    "    corpus.append(bow)\n",
    "    bow_dict.update({i:bow})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = datetime.datetime.now()\n",
    "damodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=20)\n",
    "b = datetime.datetime.now()\n",
    "c = b - a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "joblib.dump(damodel, 'LDA_model_new.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "damodel = joblib.load('LDA_model_new.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get LDA topics:\n",
    "for i in damodel.print_topics(num_topics=50, num_words=10):\n",
    "    print(i)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_date_topic_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 0\n",
    "n = 0\n",
    "for titleID, timestamp in np.bitwise_and(:\n",
    "    topics = damodel.get_document_topics(bow_dict[titleID])\n",
    "    max_prob = 0\n",
    "    max_t = 0\n",
    "    for t, prob in topics:\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_t = t\n",
    "    pol = pos_neg_groups[(titleID, timestamp)]['pol']\n",
    "#     print('pos: ', pos, 'neg: ', neg)\n",
    "    label = 'neu'\n",
    "    if pol < -0.2:\n",
    "        lebel = 'neg'\n",
    "        final_date_topic_dict.update({(titleID, timestamp):{'maxTopicID': max_t, 'label': 'neg', 'topicProb': max_prob, 'allTopicsWithProb': topics}})\n",
    "    elif pol > 0.2:\n",
    "        label = 'pos'\n",
    "        final_date_topic_dict.update({(titleID, timestamp):{'maxTopicID': max_t, 'label': 'pos', 'topicProb': max_prob, 'allTopicsWithProb': topics}})\n",
    "    else:\n",
    "        final_date_topic_dict.update({(titleID, timestamp):{'maxTopicID': max_t, 'label': 'neu', 'topicProb': max_prob, 'allTopicsWithProb': topics}})\n",
    "\n",
    "print('pos: ', p)\n",
    "print('neg: ', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_date_topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "d = 0\n",
    "for k,v in final_date_topic_dict.keys():\n",
    "    if final_date_topic_dict[(k,v)]['label'] == 'pos':\n",
    "        c+=1\n",
    "    elif  final_date_topic_dict[(k,v)]['label'] == 'neg':\n",
    "        d+=1\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_date_label_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for docID, timestamp in final_date_topic_dict:\n",
    "    max_topic = final_date_topic_dict[(docID, timestamp)]['maxTopicID']\n",
    "    if (max_topic, timestamp) in topic_date_label_dict:\n",
    "        topic_date_label_dict[(max_topic, timestamp)].append(final_date_topic_dict[(docID, timestamp)]['label'])\n",
    "    else:\n",
    "        topic_date_label_dict[(max_topic, timestamp)] = [final_date_topic_dict[(docID, timestamp)]['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_date_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for topicID, timestamp in topic_date_label_dict:\n",
    "    labels = topic_date_label_dict[(topicID, timestamp)]\n",
    "    count = len(labels)\n",
    "    counter = Counter(labels)\n",
    "    neu = counter['neu']/(0. + count)\n",
    "    pos = counter['pos']/(0. + count)\n",
    "    neg = counter['neg']/(0. + count)\n",
    "    label = 'neu'\n",
    "    if pos > neg:\n",
    "        topic_date_label_dict.update({(topicID, timestamp):'pos'})\n",
    "    elif neg > pos:\n",
    "        topic_date_label_dict.update({(topicID, timestamp):'neg'})\n",
    "    else:\n",
    "        topic_date_label_dict.update({(topicID, timestamp):'neu'})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_date_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for label in topic_date_label_dict.values():\n",
    "    if(label == 'pos'):\n",
    "        i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_date_count_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group topic daily counts\n",
    "for titleID, timestamp in date_topic_groups.keys():\n",
    "    j = len(date_topic_groups[(titleID, timestamp)])\n",
    "    \n",
    "    doc_date_count_dict.update({(titleID, timestamp):{'posts':date_topic_groups[(titleID, timestamp)], 'count': j}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for titleID, timestamp in doc_date_count_dict:\n",
    "    topics = damodel.get_document_topics(bow_dict[titleID])\n",
    "    max_prob = 0\n",
    "    max_t = 0\n",
    "    for t, prob in topics:\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_t = t\n",
    "    \n",
    "       \n",
    "    doc_date_count_dict.update({(titleID, timestamp):{'maxTopicID': max_t, 'count':doc_date_count_dict[(titleID, timestamp)]['count'], 'allTopicsWithProb': topics}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_date_count_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for docID, timestamp in doc_date_count_dict:\n",
    "    max_topic = doc_date_count_dict[(docID, timestamp)]['maxTopicID']\n",
    "    if (max_topic, timestamp) in topic_date_count_dict:\n",
    "        topic_date_count_dict[(max_topic, timestamp)]+=doc_date_count_dict[(docID, timestamp)]['count']\n",
    "    else:\n",
    "        topic_date_count_dict[(max_topic, timestamp)] = doc_date_count_dict[(docID, timestamp)]['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_date_count_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
